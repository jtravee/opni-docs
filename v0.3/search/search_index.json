{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Opni = AIOps for Kubernetes + logging + monitoring. It currently features log anomaly detection - simply ship your logs to Opni and its AI models will automatically learn and identify anomalous behavior in your control plane, etcd, and applications logs. Metrics and event anomaly detection to come soon! opnictl We have deprecated the opnictl CLI and are currently reviewing it. It should no longer be used starting in v0.2. Installing Opni \u00b6 For a complete quickstart including cluster setup, see Quickstart For a basic installation on an existing cluster with no persistent storage and no GPU, see Basic Installation For a fully customizable installation on an existing cluster (some manual configuration required), see Advanced Installation","title":"Opni - AIOps for Kubernetes"},{"location":"#installing-opni","text":"For a complete quickstart including cluster setup, see Quickstart For a basic installation on an existing cluster with no persistent storage and no GPU, see Basic Installation For a fully customizable installation on an existing cluster (some manual configuration required), see Advanced Installation","title":"Installing Opni"},{"location":"architecture/highlevel/","text":"At a high level Opni is made up of a number of microservices communicating via NATS. Logs are persisted in Opendistro Elasticsearch and AI assets are stored in S3. Log ingest \u00b6 Opni ingests logs through a http receiver for logs shipped from fluentd. GPU service (optional) \u00b6 Opni requires an NVIDIA GPU - we recommend at least a k80 - to learn and inference on your cluster's workload logs.","title":"High-level Architecture"},{"location":"architecture/highlevel/#log-ingest","text":"Opni ingests logs through a http receiver for logs shipped from fluentd.","title":"Log ingest"},{"location":"architecture/highlevel/#gpu-service-optional","text":"Opni requires an NVIDIA GPU - we recommend at least a k80 - to learn and inference on your cluster's workload logs.","title":"GPU service (optional)"},{"location":"configuration/elasticsearch/","text":"The deployment of the Elasticsearch cluster is a subsection of the OpniCluster resource. It deploys an Elasticsearch cluster and Kibana for visualization. The Opni controller will also create indices and configuration in the Elasticsearch cluster. example.yaml apiVersion : opni.io/v1beta1 kind : OpniCluster metadata : name : example namespace : opni spec : elastic : version : 1.13.2 workloads : master : resources : requests : memory : 1Gi limits : memory : 1Gi data : replicas : 2 resources : requests : memory : 2Gi limits : memory : 2Gi Custom Resource Specs \u00b6 ElasticSpec \u00b6 Field Required Type Description version No string Version of the Opendistro image to use. Defaults to latest workloads No ElasticWorkloadSpec Configure the Elasticsearch cluster components defaultRepo No string Image repo to use for Elasticsearch images. Defaults to docker.io/amazon image No ImageSpec Explicit configuration for the Elasticsearch image kibanaImage No ImageSpec Explicit configuration for the Kibana image persistence No PersistenceSpec Configure the persistence for Elasticsearch data configSecret No LocalObjectReference Reference to a secret containing logging.yml with the Elasticsearch logging config adminPasswordFrom No SecretKeySelector Secret key selector pointing to the admin password to use. If not set a password will be autogenerated and stored in the password key in a secret called opni-es-password ElasticWorkloadSpec \u00b6 Field Required Type Description master ElasticWorkloadOptions Configuration for the Elasticsearch master node StatefulSet data ElasticWorkloadOptions Configuration for the Elasticsearch data node StatefulSet client ElasticWorkloadOptions Configuration for the Elasticsearch client node Deployment kibana ElasticWorkloadOptions Configuration for the Kibana Deployment ElasticWorkloadOptions \u00b6 Field Required Type Description replicas No int Number of replicas to deploy. Defaults to 1 resources No ResourceRequirements Resources for the workload. The resources are used to calculate the Java memory options. If a memory limit exists this is used, if there is a memory request but no limit that is used, otherwise the Java optioms default to a heap size of 512MB affinity No Affinity Affinity settings for the workload pods nodeSelector No map NodeSelector for the workload pods. If this exists it will override the globalNodeSelector tolerations No Toleration array Tolerations for the workload. These will be combined with the globalTolerations (if any) ImageSpec \u00b6 Field Required Type Description image No string Explicit CRI image to use imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image PersistenceSpec \u00b6 Field Required Type Description enabled No bool Whether persistent storage is enabled. Defaults to false storageClassName No string If persistent storage is enabled, the name of the StorageClass to use. If not set will use the default StorageClass accessModes No string array An array of the access modes the volume supports request No string The size of the volume to request. Defaults to 10Gi","title":"Elasticsearch"},{"location":"configuration/elasticsearch/#custom-resource-specs","text":"","title":"Custom Resource Specs"},{"location":"configuration/elasticsearch/#elasticspec","text":"Field Required Type Description version No string Version of the Opendistro image to use. Defaults to latest workloads No ElasticWorkloadSpec Configure the Elasticsearch cluster components defaultRepo No string Image repo to use for Elasticsearch images. Defaults to docker.io/amazon image No ImageSpec Explicit configuration for the Elasticsearch image kibanaImage No ImageSpec Explicit configuration for the Kibana image persistence No PersistenceSpec Configure the persistence for Elasticsearch data configSecret No LocalObjectReference Reference to a secret containing logging.yml with the Elasticsearch logging config adminPasswordFrom No SecretKeySelector Secret key selector pointing to the admin password to use. If not set a password will be autogenerated and stored in the password key in a secret called opni-es-password","title":"ElasticSpec"},{"location":"configuration/elasticsearch/#elasticworkloadspec","text":"Field Required Type Description master ElasticWorkloadOptions Configuration for the Elasticsearch master node StatefulSet data ElasticWorkloadOptions Configuration for the Elasticsearch data node StatefulSet client ElasticWorkloadOptions Configuration for the Elasticsearch client node Deployment kibana ElasticWorkloadOptions Configuration for the Kibana Deployment","title":"ElasticWorkloadSpec"},{"location":"configuration/elasticsearch/#elasticworkloadoptions","text":"Field Required Type Description replicas No int Number of replicas to deploy. Defaults to 1 resources No ResourceRequirements Resources for the workload. The resources are used to calculate the Java memory options. If a memory limit exists this is used, if there is a memory request but no limit that is used, otherwise the Java optioms default to a heap size of 512MB affinity No Affinity Affinity settings for the workload pods nodeSelector No map NodeSelector for the workload pods. If this exists it will override the globalNodeSelector tolerations No Toleration array Tolerations for the workload. These will be combined with the globalTolerations (if any)","title":"ElasticWorkloadOptions"},{"location":"configuration/elasticsearch/#imagespec","text":"Field Required Type Description image No string Explicit CRI image to use imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image","title":"ImageSpec"},{"location":"configuration/elasticsearch/#persistencespec","text":"Field Required Type Description enabled No bool Whether persistent storage is enabled. Defaults to false storageClassName No string If persistent storage is enabled, the name of the StorageClass to use. If not set will use the default StorageClass accessModes No string array An array of the access modes the volume supports request No string The size of the volume to request. Defaults to 10Gi","title":"PersistenceSpec"},{"location":"configuration/gpuadapter/","text":"Warning The GPU Adapter is currently experimental and is behind a feature gate. To turn this on you need to run the Opni Manager with the following argument --feature-gates=GPUOperator=true The Opni Manager can assist with configuring NVIDIA GPU drivers and runtimes. To do this it uses an embedded NVIDIA GPU Operator with a wrapper. The GPU operator requires Node Feature Discovery running as a prerequisite. If this is not already deployed it can be run with the following command: kubectl apply -f https://raw.githubusercontent.com/rancher/opni/main/deploy/examples/nfd_aio.yaml It currently supports the following Kubernetes distributions: rke k3s (v1.22.2+k3s1 or later) auto (auto detection) And the following container runtimes: docker containerd crio auto (auto detection) example.yaml apiVersion : opni.io/v1beta1 kind : GpuPolicyAdapter metadata : name : example-adapter spec : {} Custom Resource Specs \u00b6 GpuPolicyAdapterSpec \u00b6 Field Required Type Description containerRuntime No string The container runtime the host is using. Must be one of docker, containerd, crio, or auto. Defaults to auto kubernetesProvider No string The kubernetes distribution. Must be one of k3s, rke2, rke, auto, or none. Defaults to auto images No ImagesSpec Overrides for the images used by the operator (primarily for airgapped scenarios) vgpu No VGPUSpec Additional config required if using vGPUs template No nvidia ClusterPolicySpec Overrides for the ClusterPolicy created by the GpuPolicyAdapter. Details about the fields can be found in the NVIDIA documentation or by reviewing the NVIDIA code ImagesSpec \u00b6 Field Required Type Description driver No string Driver image driverManager No string Driver Manager image dcgm No string DCGM image dcgmExporter No string DCGM Exporter image devicePlugin No string Device Plugin image gfd No string GFD image initContainer No string InitContainer image for deployed workloads toolkit No string Toolkit image validator No string Validator image migManager No string MIG Manager image Info Details about the images can be found in the NVIDIA airgap documentation VGPUSpec \u00b6 Field Required Type Description licenseConfigMap Yes string Name of the config map that contains the vGPU license file licenseServerKind Yes string The type of vGPU license. Must be one of nls, or legacy","title":"GPU Policy Adapter"},{"location":"configuration/gpuadapter/#custom-resource-specs","text":"","title":"Custom Resource Specs"},{"location":"configuration/gpuadapter/#gpupolicyadapterspec","text":"Field Required Type Description containerRuntime No string The container runtime the host is using. Must be one of docker, containerd, crio, or auto. Defaults to auto kubernetesProvider No string The kubernetes distribution. Must be one of k3s, rke2, rke, auto, or none. Defaults to auto images No ImagesSpec Overrides for the images used by the operator (primarily for airgapped scenarios) vgpu No VGPUSpec Additional config required if using vGPUs template No nvidia ClusterPolicySpec Overrides for the ClusterPolicy created by the GpuPolicyAdapter. Details about the fields can be found in the NVIDIA documentation or by reviewing the NVIDIA code","title":"GpuPolicyAdapterSpec"},{"location":"configuration/gpuadapter/#imagesspec","text":"Field Required Type Description driver No string Driver image driverManager No string Driver Manager image dcgm No string DCGM image dcgmExporter No string DCGM Exporter image devicePlugin No string Device Plugin image gfd No string GFD image initContainer No string InitContainer image for deployed workloads toolkit No string Toolkit image validator No string Validator image migManager No string MIG Manager image Info Details about the images can be found in the NVIDIA airgap documentation","title":"ImagesSpec"},{"location":"configuration/gpuadapter/#vgpuspec","text":"Field Required Type Description licenseConfigMap Yes string Name of the config map that contains the vGPU license file licenseServerKind Yes string The type of vGPU license. Must be one of nls, or legacy","title":"VGPUSpec"},{"location":"configuration/logadapter/","text":"The LogAdapter custom resource simplifies the configuration of log shipping for a range of Kubernetes distributions. It builds on the Banzai Cloud logging operator to do this. It currently supports the following distributions rke rke2 k3s aks eks gke example.yaml apiVersion : opni.io/v1beta1 kind : LogAdapter metadata : name : example-adapter spec : provider : rke opniCluster : name : example-cluster namespace : opni Custom Resource Specs \u00b6 LogAdapterSpec \u00b6 Field Required Type Description provider Yes string One of the supported distributions opniCluster Yes OpniClusterNameSpec A reference to an installed OpniCluster containerLogDir No string Path to the container logs on the host. Defaults to /var/lib/docker/containers seLinuxEnabled No bool Enable SELinux support. Defaults to false rootFluentConfig No FluentConfigSpec Fluentd and Fluentbit config for the base conatiner log shipping. If this is absent it it creates default Banzaicloud configs, with a liveness probe for fluentd, and uses the Rancher logging images fluentConfig No FluentConfigSpec Fluent config for additional Logging object for K3S and cloud providers. Defaults as per rootFluentConfig, along with additional sane default file locations for the specific distribution aks No AKSSpec AKS config; should be empty eks No EKSSpec EKS config; should be empty gke No GKESpec GKE config; should be empty rke No RKESpec RKE specific config k3s No K3SSpec K3s specific config rke2 No RKE2Spec RKE2 specific config N.B. If a distribution spec is set it must match the provider field OpniClusterNameSpec \u00b6 Field Required Type Description name Yes string Name of the opnicluster to link the LogAdapter to namespace No string Name of the namespace the opnicluster is in. Defaults to default FluentConfigSpec \u00b6 Field Required Type Description fluentbit No FluentbitSpec Uses Banzaicloud defaults with the image spec set to the Rancher logging images fluentd No FluentdSpec Uses Banzaicloud defaults with the image spec set to the Rancher logging images, and a liveness probe configured RKESpec \u00b6 Field Required Type Description logLevel No string Log level for the fluentbit aggregator. Defaults to info K3SSpec \u00b6 Field Required Type Description containerEngine No string Must be either systemd or openrc . Defaults to systemd logPath No string For systemd this is the path to the journalctl log location. Defaults to /var/log/journal . For openrc this is the path to the service log file. Defaults to /var/log/k3s.log RKE2Spec \u00b6 Field Required Type Description logPath No string This is the path to the journalctl log location. Defaults to /var/log/journal","title":"LogAdapter"},{"location":"configuration/logadapter/#custom-resource-specs","text":"","title":"Custom Resource Specs"},{"location":"configuration/logadapter/#logadapterspec","text":"Field Required Type Description provider Yes string One of the supported distributions opniCluster Yes OpniClusterNameSpec A reference to an installed OpniCluster containerLogDir No string Path to the container logs on the host. Defaults to /var/lib/docker/containers seLinuxEnabled No bool Enable SELinux support. Defaults to false rootFluentConfig No FluentConfigSpec Fluentd and Fluentbit config for the base conatiner log shipping. If this is absent it it creates default Banzaicloud configs, with a liveness probe for fluentd, and uses the Rancher logging images fluentConfig No FluentConfigSpec Fluent config for additional Logging object for K3S and cloud providers. Defaults as per rootFluentConfig, along with additional sane default file locations for the specific distribution aks No AKSSpec AKS config; should be empty eks No EKSSpec EKS config; should be empty gke No GKESpec GKE config; should be empty rke No RKESpec RKE specific config k3s No K3SSpec K3s specific config rke2 No RKE2Spec RKE2 specific config N.B. If a distribution spec is set it must match the provider field","title":"LogAdapterSpec"},{"location":"configuration/logadapter/#opniclusternamespec","text":"Field Required Type Description name Yes string Name of the opnicluster to link the LogAdapter to namespace No string Name of the namespace the opnicluster is in. Defaults to default","title":"OpniClusterNameSpec"},{"location":"configuration/logadapter/#fluentconfigspec","text":"Field Required Type Description fluentbit No FluentbitSpec Uses Banzaicloud defaults with the image spec set to the Rancher logging images fluentd No FluentdSpec Uses Banzaicloud defaults with the image spec set to the Rancher logging images, and a liveness probe configured","title":"FluentConfigSpec"},{"location":"configuration/logadapter/#rkespec","text":"Field Required Type Description logLevel No string Log level for the fluentbit aggregator. Defaults to info","title":"RKESpec"},{"location":"configuration/logadapter/#k3sspec","text":"Field Required Type Description containerEngine No string Must be either systemd or openrc . Defaults to systemd logPath No string For systemd this is the path to the journalctl log location. Defaults to /var/log/journal . For openrc this is the path to the service log file. Defaults to /var/log/k3s.log","title":"K3SSpec"},{"location":"configuration/logadapter/#rke2spec","text":"Field Required Type Description logPath No string This is the path to the journalctl log location. Defaults to /var/log/journal","title":"RKE2Spec"},{"location":"configuration/nats/","text":"Opni requires a NATS cluster to enable communication between services. The deployment of a NATS cluster is configured by a subsection of the OpniCluster resource. example.yaml apiVersion : opni.io/v1beta1 kind : OpniCluster metadata : name : example namespace : opni spec : nats : authMethod : nkey Custom Resource Specs \u00b6 NatsSpec \u00b6 Field Required Type Description authMethod No string Must be either username or nkey . Defaults to nkey replicas No int Number of NATS replicas to deploy (should be an odd number). Defaults to 3 username No string Username to use with the username auth method. If not set defaults to nats-user passwordFrom No SecretKeySelector Secret key containing the password to use. If not set then a random password will be generated and used nodeSelector No map NodeSelector for the cluster pods. If this exists it will override the globalNodeSelector tolerations No Toleration array Tolerations for the cluster pods. These will be combined with the globalTolerations (if any)","title":"NATS"},{"location":"configuration/nats/#custom-resource-specs","text":"","title":"Custom Resource Specs"},{"location":"configuration/nats/#natsspec","text":"Field Required Type Description authMethod No string Must be either username or nkey . Defaults to nkey replicas No int Number of NATS replicas to deploy (should be an odd number). Defaults to 3 username No string Username to use with the username auth method. If not set defaults to nats-user passwordFrom No SecretKeySelector Secret key containing the password to use. If not set then a random password will be generated and used nodeSelector No map NodeSelector for the cluster pods. If this exists it will override the globalNodeSelector tolerations No Toleration array Tolerations for the cluster pods. These will be combined with the globalTolerations (if any)","title":"NatsSpec"},{"location":"configuration/opnicluster/","text":"The OpniCluster resource controls the deployment of the Opni microservices, along with prerequisite infrastructure (such as NATS). This is the main resource used for deploying Opni. example.yaml apiVersion : opni.io/v1beta1 kind : OpniCluster metadata : name : example namespace : opni spec : version : v0.1.3 deployLogCollector : true services : inference : enabled : true pretrainedModels : - name : control-plane gpuController : enabled : false elastic : version : 1.13.2 nats : authMethod : nkey s3 : internal : {} Custom Resource Specs \u00b6 OpniClusterSpec \u00b6 Field Required Type Description version No string The version of the Opni service images to deploy. Defaults to latest defaultRepo No string Docker repo to use for the Opni service images. Defaults to docker.io/rancher services No ServicesSpec Configuration for the individual Opni services elastic No ElasticSpec Configures the Opendistro Elasticsearch cluster deployed along with Opni nats No NatsSpec Configures the NATS cluster deployed along with Opni s3 Yes S3Spec Configures the S3 storage used for persisting the AI models deployLogCollector No bool Whether the Opni ClusterFlow and ClusterOutput should be deployed. Defaults to true. See Log Shipping for more details globalNodeSelector No map A node selector that will get applied to all Opni components and deployed infrastructure globalTolerations No Toleration array Tolerations that will get applied to all Opni components and infrastructure nulogHyperParameters No map Overrides for the default nulog hyperparameters Warning The default hyperparameters should not be overridden unless there is a specific reason to ServicesSpec \u00b6 Field Required Type Description drain No DrainServiceSpec Configuration for the Opni Drain service inference No InferenceServiceSpec Configuration for the Opni Nulog Inference services preprocessing No PreprocessingServiceSpec Configuration for the Opni Preprocessing service payloadReceiver No PayloadReceiverServiceSpec Configuration for the Opni HTTP Payload Receiver service gpuController No GPUControllerServiceSpec Configuration for the optional GPU service metrics No MetricsServiceSpec Configuration for the metrics anomaly detection service insights No InsightsServiceSpec Configuration for the insights api service ui No UIServiceSpec Configuration for the custom Opni UI service DrainServiceSpec \u00b6 Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any) InferenceServiceSpec \u00b6 Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any) pretrainedModels No LocalObjectReference array A list of pretrained models to deploy. Object references should point to a PretrainedModel resource PreprocessingServiceSpec \u00b6 Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any) PayloadReceiverServiceSpec \u00b6 Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any) GPUControllerServiceSpec \u00b6 Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any) MetricsServiceSpec \u00b6 Note The metrics anomaly service is currently experimental Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any) prometheus No PrometheusReference Reference to a prometheus-operator Prometheus resource. If this is provided ServiceMonitor and PrometheusRule resources will be created prometheusEndpoint No string Endpoint of Prometheus cluster for the cluster. This is required if the prometheus reference is not provided, or the referenced Prometheus object doens't include an externalURL InsightsServiceSpec \u00b6 Note The insights service is currently experimental. It provides an API that the custom UI uses. Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any) UIServiceSpec \u00b6 Note The UI is currently experimental. It is not currently exposed outside the cluster, however changes to the Kubernetes Service will not be reconciled, so it can be updated to be a LoadBalancer or NodePort. Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any) PrometheusReference \u00b6 Field Required Type Description name Yes string The name of the Prometheus resource namespace Yes string The namespace the Prometheus resource is in","title":"OpniCluster"},{"location":"configuration/opnicluster/#custom-resource-specs","text":"","title":"Custom Resource Specs"},{"location":"configuration/opnicluster/#opniclusterspec","text":"Field Required Type Description version No string The version of the Opni service images to deploy. Defaults to latest defaultRepo No string Docker repo to use for the Opni service images. Defaults to docker.io/rancher services No ServicesSpec Configuration for the individual Opni services elastic No ElasticSpec Configures the Opendistro Elasticsearch cluster deployed along with Opni nats No NatsSpec Configures the NATS cluster deployed along with Opni s3 Yes S3Spec Configures the S3 storage used for persisting the AI models deployLogCollector No bool Whether the Opni ClusterFlow and ClusterOutput should be deployed. Defaults to true. See Log Shipping for more details globalNodeSelector No map A node selector that will get applied to all Opni components and deployed infrastructure globalTolerations No Toleration array Tolerations that will get applied to all Opni components and infrastructure nulogHyperParameters No map Overrides for the default nulog hyperparameters Warning The default hyperparameters should not be overridden unless there is a specific reason to","title":"OpniClusterSpec"},{"location":"configuration/opnicluster/#servicesspec","text":"Field Required Type Description drain No DrainServiceSpec Configuration for the Opni Drain service inference No InferenceServiceSpec Configuration for the Opni Nulog Inference services preprocessing No PreprocessingServiceSpec Configuration for the Opni Preprocessing service payloadReceiver No PayloadReceiverServiceSpec Configuration for the Opni HTTP Payload Receiver service gpuController No GPUControllerServiceSpec Configuration for the optional GPU service metrics No MetricsServiceSpec Configuration for the metrics anomaly detection service insights No InsightsServiceSpec Configuration for the insights api service ui No UIServiceSpec Configuration for the custom Opni UI service","title":"ServicesSpec"},{"location":"configuration/opnicluster/#drainservicespec","text":"Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any)","title":"DrainServiceSpec"},{"location":"configuration/opnicluster/#inferenceservicespec","text":"Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any) pretrainedModels No LocalObjectReference array A list of pretrained models to deploy. Object references should point to a PretrainedModel resource","title":"InferenceServiceSpec"},{"location":"configuration/opnicluster/#preprocessingservicespec","text":"Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any)","title":"PreprocessingServiceSpec"},{"location":"configuration/opnicluster/#payloadreceiverservicespec","text":"Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any)","title":"PayloadReceiverServiceSpec"},{"location":"configuration/opnicluster/#gpucontrollerservicespec","text":"Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any)","title":"GPUControllerServiceSpec"},{"location":"configuration/opnicluster/#metricsservicespec","text":"Note The metrics anomaly service is currently experimental Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any) prometheus No PrometheusReference Reference to a prometheus-operator Prometheus resource. If this is provided ServiceMonitor and PrometheusRule resources will be created prometheusEndpoint No string Endpoint of Prometheus cluster for the cluster. This is required if the prometheus reference is not provided, or the referenced Prometheus object doens't include an externalURL","title":"MetricsServiceSpec"},{"location":"configuration/opnicluster/#insightsservicespec","text":"Note The insights service is currently experimental. It provides an API that the custom UI uses. Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any)","title":"InsightsServiceSpec"},{"location":"configuration/opnicluster/#uiservicespec","text":"Note The UI is currently experimental. It is not currently exposed outside the cluster, however changes to the Kubernetes Service will not be reconciled, so it can be updated to be a LoadBalancer or NodePort. Field Required Type Description image No string Explicit override for the image to use for the service imagePullPolicy No string Image pull policy. One of Always, Never, IfNotPresent. Defaults to IfNotPresent imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image enabled No bool Enable the service. Defaults to true nodeSelector No map A node selector that will be applied to the Drain service. This overrides the globalNodeSelector field tolerations No Toleration array Tolerations for the service. These will be added to the tolerations in globalTolerations (if any)","title":"UIServiceSpec"},{"location":"configuration/opnicluster/#prometheusreference","text":"Field Required Type Description name Yes string The name of the Prometheus resource namespace Yes string The namespace the Prometheus resource is in","title":"PrometheusReference"},{"location":"configuration/pretrainedmodel/","text":"The Opni system has the ability to make use of pretrained nulog models. Currently we provide a pretrained model for the Kubernetes control plane. The configuration for pretrained models is controlled by the PretrainedModel resource. This resource is then referenced by the OpniCluster resource to deploy the model example.yaml apiVersion : opni.io/v1beta1 kind : PretrainedModel metadata : name : control-plane namespace : opni spec : source : http : url : \"https://opni-public.s3.us-east-2.amazonaws.com/pretrain-models/control-plane-model-v0.1.2.zip\" hyperparameters : modelThreshold : \"0.6\" minLogTokens : 4 isControlPlane : \"true\" Custom Resource Specs \u00b6 PretrainedModelSpec \u00b6 Field Required Type Description source yes ModelSource A reference to the location of the pretrained model files hyperparameters yes map An optional map of hyperparameters to pass to the model. Values must be integers or strings. Keys must be strings ModelSource \u00b6 Field Required Type Description http No HTTPSource A reference to a http location service the model tarball. If this is not provided a container source must be. container No ContainerSource A reference to a CRI image containing a pretrained model at /model/model.tar.gz . If this is not provided a http source must be HTTPSource \u00b6 Field Required Type Description url Yes string The URL to download the pretrained model from ContainerSource \u00b6 Field Required Type Description image Yes string The CRI image to use imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image","title":"PretrainedModel"},{"location":"configuration/pretrainedmodel/#custom-resource-specs","text":"","title":"Custom Resource Specs"},{"location":"configuration/pretrainedmodel/#pretrainedmodelspec","text":"Field Required Type Description source yes ModelSource A reference to the location of the pretrained model files hyperparameters yes map An optional map of hyperparameters to pass to the model. Values must be integers or strings. Keys must be strings","title":"PretrainedModelSpec"},{"location":"configuration/pretrainedmodel/#modelsource","text":"Field Required Type Description http No HTTPSource A reference to a http location service the model tarball. If this is not provided a container source must be. container No ContainerSource A reference to a CRI image containing a pretrained model at /model/model.tar.gz . If this is not provided a http source must be","title":"ModelSource"},{"location":"configuration/pretrainedmodel/#httpsource","text":"Field Required Type Description url Yes string The URL to download the pretrained model from","title":"HTTPSource"},{"location":"configuration/pretrainedmodel/#containersource","text":"Field Required Type Description image Yes string The CRI image to use imagePullSecrets No LocalObjectReference array List of secrets in the same namespace to use for pulling the image","title":"ContainerSource"},{"location":"configuration/s3/","text":"Opni requires an S3 endpoint to store the AI models for the Drain and Inference services. This can be endpoint for an external S3 compatible API, or Opni can deploy a SeaweedFS pod to serve the S3 API. example.yaml apiVersion : opni.io/v1beta1 kind : OpniCluster metadata : name : example namespace : opni spec : s3 : internal : {} Custom Resource Specs \u00b6 S3Spec \u00b6 Field Required Type Description internal No InternalSpec If set will deploy an internal S3 endpoint to use external No ExternalSpec The reference to the external S3 compatible API to use nulogS3Bucket No string Name of the S3 bucket to use for the Nulog model. Defaults to opni-nulog-models drainS3Bucket No string Name of the S3 bucket to use for the Drain model. Defaults to opni-drain-model InternalSpec \u00b6 Field Required Type Description persistence No PersistenceSpec If set SeaweedFS will be configured to use persistent storage PersistenceSpec \u00b6 Field Required Type Description enabled No bool Whether persistent storage is enabled. Defaults to false storageClassName No string If persistent storage is enabled, the name of the StorageClass to use. If not set will use the default StorageClass accessModes No string array An array of the access modes the volume supports request No string The size of the volume to request. Defaults to 10Gi ExternalSpec \u00b6 Field Required Type Description endpoint Yes string The external S3 endpoint URL credentials Yes SecretReference Reference to a secret containing the S3 credentials. It must have accessKey and secretKey items","title":"S3"},{"location":"configuration/s3/#custom-resource-specs","text":"","title":"Custom Resource Specs"},{"location":"configuration/s3/#s3spec","text":"Field Required Type Description internal No InternalSpec If set will deploy an internal S3 endpoint to use external No ExternalSpec The reference to the external S3 compatible API to use nulogS3Bucket No string Name of the S3 bucket to use for the Nulog model. Defaults to opni-nulog-models drainS3Bucket No string Name of the S3 bucket to use for the Drain model. Defaults to opni-drain-model","title":"S3Spec"},{"location":"configuration/s3/#internalspec","text":"Field Required Type Description persistence No PersistenceSpec If set SeaweedFS will be configured to use persistent storage","title":"InternalSpec"},{"location":"configuration/s3/#persistencespec","text":"Field Required Type Description enabled No bool Whether persistent storage is enabled. Defaults to false storageClassName No string If persistent storage is enabled, the name of the StorageClass to use. If not set will use the default StorageClass accessModes No string array An array of the access modes the volume supports request No string The size of the volume to request. Defaults to 10Gi","title":"PersistenceSpec"},{"location":"configuration/s3/#externalspec","text":"Field Required Type Description endpoint Yes string The external S3 endpoint URL credentials Yes SecretReference Reference to a secret containing the S3 credentials. It must have accessKey and secretKey items","title":"ExternalSpec"},{"location":"deployment/advanced/","text":"Prerequisites: \u00b6 Opni requires Cert-Manager to run. Follow the official instructions to install Cert-Manager. Clone the Opni repo \u00b6 $ git clone https://github.com/rancher/opni-docs.git # Or by using the github CLI: $ gh repo clone rancher/opni Install the Opni operator \u00b6 Run the following command to install the Opni operator with Kustomize: $ kubectl create -k config/default Wait for the operator to be ready \u00b6 $ kubectl wait --timeout=300s --for=condition=available deploy/opni-controller-manager -n opni-system Configure and install Opni components \u00b6 Configure which Opni components will be installed by editing deploy/kustomization.yaml . This file specifies locations of other YAML files containing the required Opni deployment configuration, as well as example entries for optional features. Some optional features require additional configuration in separate files. After configuration is complete, install them using Kustomize: $ kubectl create -k deploy After the components are installed, monitor pods running in the opni namespace and wait for them to be ready. This may take a few minutes. If GPU support is enabled, several GPU Operator pods will be running in the gpu-operator-resources namespace. It takes several minutes for the GPU operator to configure your cluster. Your container runtime will be restarted during this process, so it is normal to experience brief connectivity issues during this time. See Cluster GPU Configuration for more details regarding GPU operator configuration.","title":"Advanced Installation"},{"location":"deployment/advanced/#prerequisites","text":"Opni requires Cert-Manager to run. Follow the official instructions to install Cert-Manager.","title":"Prerequisites:"},{"location":"deployment/advanced/#clone-the-opni-repo","text":"$ git clone https://github.com/rancher/opni-docs.git # Or by using the github CLI: $ gh repo clone rancher/opni","title":"Clone the Opni repo"},{"location":"deployment/advanced/#install-the-opni-operator","text":"Run the following command to install the Opni operator with Kustomize: $ kubectl create -k config/default","title":"Install the Opni operator"},{"location":"deployment/advanced/#wait-for-the-operator-to-be-ready","text":"$ kubectl wait --timeout=300s --for=condition=available deploy/opni-controller-manager -n opni-system","title":"Wait for the operator to be ready"},{"location":"deployment/advanced/#configure-and-install-opni-components","text":"Configure which Opni components will be installed by editing deploy/kustomization.yaml . This file specifies locations of other YAML files containing the required Opni deployment configuration, as well as example entries for optional features. Some optional features require additional configuration in separate files. After configuration is complete, install them using Kustomize: $ kubectl create -k deploy After the components are installed, monitor pods running in the opni namespace and wait for them to be ready. This may take a few minutes. If GPU support is enabled, several GPU Operator pods will be running in the gpu-operator-resources namespace. It takes several minutes for the GPU operator to configure your cluster. Your container runtime will be restarted during this process, so it is normal to experience brief connectivity issues during this time. See Cluster GPU Configuration for more details regarding GPU operator configuration.","title":"Configure and install Opni components"},{"location":"deployment/basic/","text":"1) Install Cert Manager \u00b6 Cert Manager needs to be installed for the operator. This can be installed with the following command: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.4/cert-manager.yaml 2) Install prerequisites \u00b6 Next the CRDs and RBAC need to be installed for the operator: kubectl apply -f https://raw.githubusercontent.com/rancher/opni/main/deploy/manifests/00_crds.yaml kubectl apply -f https://raw.githubusercontent.com/rancher/opni/main/deploy/manifests/01_rbac.yaml 3) Install the operator \u00b6 Next install the operator into the cluster: kubectl apply -f https://raw.githubusercontent.com/rancher/opni/main/deploy/manifests/10_operator.yaml 4) Create the OpniCluster resource \u00b6 Note The operator deployment should be ready before applying the OpniCluster, otherwise the admission webhook will fail. Create the Opni cluster: kubectl apply -f https://raw.githubusercontent.com/rancher/opni/main/deploy/manifests/20_cluster.yaml To deploy the GPU Controller service edit the resource and set spec.services.gpuController to be true . Make sure the cluster has been setup for GPU support. If Rancher Logging is not installed then log shipping will need to be setup More details about the OpniCluster custom resource can be found in the configuration page .","title":"Basic Installation"},{"location":"deployment/basic/#1-install-cert-manager","text":"Cert Manager needs to be installed for the operator. This can be installed with the following command: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.4/cert-manager.yaml","title":"1) Install Cert Manager"},{"location":"deployment/basic/#2-install-prerequisites","text":"Next the CRDs and RBAC need to be installed for the operator: kubectl apply -f https://raw.githubusercontent.com/rancher/opni/main/deploy/manifests/00_crds.yaml kubectl apply -f https://raw.githubusercontent.com/rancher/opni/main/deploy/manifests/01_rbac.yaml","title":"2) Install prerequisites"},{"location":"deployment/basic/#3-install-the-operator","text":"Next install the operator into the cluster: kubectl apply -f https://raw.githubusercontent.com/rancher/opni/main/deploy/manifests/10_operator.yaml","title":"3) Install the operator"},{"location":"deployment/basic/#4-create-the-opnicluster-resource","text":"Note The operator deployment should be ready before applying the OpniCluster, otherwise the admission webhook will fail. Create the Opni cluster: kubectl apply -f https://raw.githubusercontent.com/rancher/opni/main/deploy/manifests/20_cluster.yaml To deploy the GPU Controller service edit the resource and set spec.services.gpuController to be true . Make sure the cluster has been setup for GPU support. If Rancher Logging is not installed then log shipping will need to be setup More details about the OpniCluster custom resource can be found in the configuration page .","title":"4) Create the OpniCluster resource"},{"location":"deployment/quickstart/","text":"On a fresh VM run the quickstart script: curl -sfL https://raw.githubusercontent.com/rancher/opni-docs/main/quickstart_files/install_opni.sh | sh - This script will set up a RKE2 cluster and install Opni into it. It will also generate a control-plane anomaly that Opni will detect. Kibana UI \u00b6 To view the Kibana UI you will need to port forward it: export PATH = $PATH :/var/lib/rancher/rke2/bin kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \\ -n opni-cluster \\ port-forward --address 0 .0.0.0 svc/opni-es-kibana 5601 :5601 Open the following address in a browser [IPV4_ADDRESS]:5601 The username is admin and the password is stored in the opni-es-password secret in the opni-cluster namespace: export PATH = $PATH :/var/lib/rancher/rke2/bin kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \\ -n opni-cluster \\ get secret opni-es-password --template ={{ \"{{.data.password}}\" }} | base64 -d You must be in the Global Tenant mode if you are not already. Click on Dashboard, Opni Logs Dashboard. Additional Anomaly Injection \u00b6 Using the provided script, you can inject sample anomalies into your cluster. The script can create pods which are unschedulable, have nonexistent images, or exit with non-zero exit codes. Note: If you are not using the quickstart script, you must set the KUBECONFIG environment variable like export KUBECONFIG =[ PATH_TO_KUBECONFIG_FILE ] export PATH = $PATH : [ PATH_TO_KUBECTL_BINARY ] Then you can inject anomalies into your cluster with this command: sh < ( curl -sfL https://raw.githubusercontent.com/rancher/opni-docs/main/quickstart_files/errors_injection.sh )","title":"Quickstart"},{"location":"deployment/quickstart/#kibana-ui","text":"To view the Kibana UI you will need to port forward it: export PATH = $PATH :/var/lib/rancher/rke2/bin kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \\ -n opni-cluster \\ port-forward --address 0 .0.0.0 svc/opni-es-kibana 5601 :5601 Open the following address in a browser [IPV4_ADDRESS]:5601 The username is admin and the password is stored in the opni-es-password secret in the opni-cluster namespace: export PATH = $PATH :/var/lib/rancher/rke2/bin kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \\ -n opni-cluster \\ get secret opni-es-password --template ={{ \"{{.data.password}}\" }} | base64 -d You must be in the Global Tenant mode if you are not already. Click on Dashboard, Opni Logs Dashboard.","title":"Kibana UI"},{"location":"deployment/quickstart/#additional-anomaly-injection","text":"Using the provided script, you can inject sample anomalies into your cluster. The script can create pods which are unschedulable, have nonexistent images, or exit with non-zero exit codes. Note: If you are not using the quickstart script, you must set the KUBECONFIG environment variable like export KUBECONFIG =[ PATH_TO_KUBECONFIG_FILE ] export PATH = $PATH : [ PATH_TO_KUBECTL_BINARY ] Then you can inject anomalies into your cluster with this command: sh < ( curl -sfL https://raw.githubusercontent.com/rancher/opni-docs/main/quickstart_files/errors_injection.sh )","title":"Additional Anomaly Injection"},{"location":"setup/gpu/","text":"Automatic GPU Operator Configuration \u00b6 Opni can utilize GPU acceleration to enable log anomaly detection for workload and application logs. If you would like Opni to learn from your own workloads, follow the instructions below to configure your cluster. If GPU acceleration is not enabled, Opni can still analyze your Control Plane logs using our pretrained models. Opni bundles its own modified version of the Nvidia GPU Operator which is the recommended way to enable GPU acceleration. It may be possible to install the upstream GPU Operator via helm chart, but this is not recommended, as it will likely not work with Opni. Follow the instructions below to enable GPU acceleration using Opni's built-in GPU Operator. Prerequisites \u00b6 GPU Node Hardware Requirements: \u00b6 1x NVIDIA GPU If not using VGPU, any Quadro or Tesla GPU will work. If using VGPU, one of the following GPUs is required: Tesla M6, M10, M60 Tesla P4, P6, P40, P100 Tesla V100 Quadro RTX 6000/8000 Tesla T4 NVIDIA A10, A16, A30, A40, A100, RTX A5000/A6000 MIG is not supported when using automatic GPU Operator configuration at this time. Ubuntu 20.04. No other host OS is supported at this time. The GPU node should not have an Nvidia driver or container runtime installed. The GPU Operator handles installing all the necessary drivers automatically. Kubernetes Cluster Requirements: \u00b6 Vanilla upstream Kubernetes, or one of the following distributions: RKE K3S >= v1.22.2+k3s1 RKE2 Amazon EKS Google GKE Azure AKS Opni Operator installed Cert-Manager installed NFD installed (see deploy/examples/nfd_aio.yaml in the Opni repo) Install Custom Resources \u00b6 GPUPolicyAdapter \u00b6 Creating a GPUPolicyAdapter resource will trigger the GPU Operator to start configuring the cluster. If not using VGPU, the GPUPolicyAdapter resource does not need any customization, and can be created with an empty spec, as follows: apiVersion : opni.io/v1beta1 kind : GpuPolicyAdapter metadata : name : gpu spec : {} (note that the empty brackets after spec: are important) If using VGPU, please follow the official documentation to build the VGPU driver image, and create a GPUPolicyAdapter resource with the following spec, filling in the driver and licenseConfigMap fields: apiVersion : opni.io/v1beta1 kind : GpuPolicyAdapter metadata : name : vgpu spec : images : driver : # Image name for the VGPU driver vgpu : licenseConfigMap : # ConfigMap containing gridd.conf and the client config token licenseServerKind : nls Once the GPUPolicyAdapter resource is created, the GPU Operator will begin configuring the cluster. This takes a few minutes, and the container engine will be restarted during this process. Once all the pods in the gpu-operator-resources namespace complete, GPUs will be available for use and the Opni GPU Controller pod should start. Provider-specific Notes and Troubleshooting \u00b6 K3S \u00b6 If you are using K3S, you must use v1.22.2+k3s1 or later. This version introduces automatic detection of Nvidia container runtimes on nodes, and a large number of bugfixes related to pod lifecycle were added in upstream Kubernetes for this release. If you are using an earlier version, you may experience issues where pods become stuck in the 'Terminating' state. If this happens, force-deleting the stuck pods should help resolve the issue. RKE \u00b6 RKE uses Docker as its container engine. If you are using RKE, you should be aware of how the Nvidia runtime interacts with Docker. When the GPU operator is installed for the first time, it will create a RuntimeClass object which allows Kubernetes to use the nvidia runtime, which is configured in the docker engine itself. The RuntimeClass will look similar to this when using RKE: apiVersion : node.k8s.io/v1alpha1 kind : RuntimeClass metadata : name : nvidia handler : docker The RuntimeClass name is used in a pod spec to identify the runtime to use. It is specified using the runtimeClassName field: apiVersion : v1 kind : Pod spec : # ... runtimeClassName : nvidia # ... When using containerd for example, the handler field in the RuntimeClass is used to select a specific container runtime by name. However, dockershim does not support selecting custom container runtimes, meaning the nvidia container runtime must be set as the default. The GPU Operator will still utilize a RuntimeClass , but its handler must be set to docker to work. Otherwise, you may see an error like this: Failed to create pod sandbox: rpc error: code = Unknown desc = RuntimeHandler \"nvidia\" not supported If you encounter this error, check the handler field in the RuntimeClass and ensure it is set to docker . VGPU \u00b6 If you are using VGPUs, you should be aware of the following: Nvidia gridd does not run on the virtualization host. Instead, it runs in guest VMs - when using the GPU Operator, gridd runs inside the nvidia driver daemonset pod. The VGPU driver must match the driver on the host. When downloading the driver package from the Nvidia License Portal, you will be presented with two driver runfiles, for example: NVIDIA-Linux-x86_64-470.63-vgpu-kvm.run - Installed on the host NVIDIA-Linux-x86_64-470.63.01-grid.run - Installed on the guest (automatic with GPU operator) Ensure the correct drivers are used for the host and guest. CUDA drivers do not need to be installed on the virtualization host (and are not included by default with the VGPU driver) VGPU mediated device UUIDs are not persistent across reboots. This can cause libvirt to fail after a reboot if you are running a VM with VGPUs. To fix this: List instances on the host with virsh list --all (the instance will be shut off) Find the instance that has the VGPU. If you don't know which one it is, use the virsh edit command to view the XML configuration of each instance and search for mdev . Undefine the instance using virsh undefine and restart libvirtd. You might not be able to determine from the host whether a guest VGPU is licensed. To check license status, you should open a shell to a pod running with the nvidia container runtime and use nvidia-smi from there. On the virtualization host, you may notice that your physical GPU is bound to the nvidia driver, even though the nvidia_vgpu_vfio driver is available. The official docs are ambiguous on this, but your GPU should be bound to the nvidia driver. The guest VGPU will also be bound to the nvidia driver once the guest drivers are installed. Other Notes \u00b6 The nvidia.com/gpu resource request behaves counterintuitively. It serves as a node scheduling hint for pods, and to keep track of which/how many pods are using GPUs. nvidia.com/gpu a first-class node resource just like cpu and memory. If a pod makes a request for one, the node it is scheduled on must have that resource available. The existence of this resource request does not equate to a pod taking exclusive ownership of the device. In fact, a pod can use a GPU without requesting an nvidia.com/gpu resource at all, as long as it has the correct environment variables and runtimeClassName set. This is how the GPU Operator pods are configured - they do not request GPU resources by design, rather they are scheduled onto GPU nodes by other means and set up to use the nvidia container runtime. Regardless of nvidia.com/gpu resource requests or container runtime, GPUs will only be available if the NVIDIA_VISIBLE_DEVICES and NVIDIA_DRIVER_CAPABILITIES environment variables are set correctly. They should be set as follows: NVIDIA_VISIBLE_DEVICES=all NVIDIA_DRIVER_CAPABILITIES=compute,utility They can be set either in the pod spec, or in the docker image itself by using ENV in the Dockerfile. If you are not using the GPU operator, but you are using the nvidia device plugin daemonset by itself, be aware that you must patch the daemonset to include runtimeClassName: nvidia in its pod template. The GPU operator will do this automatically, but the daemonset does not have it set by default. The GPU Operator installs the drivers in such a way that they are not directly visible or usable by the host. As such, tools like nvidia-smi will not be available, so you will need to exec into a pod running with the nvidia container runtime to interact with a GPU. A common way to pass a GPU through to a virtual machine is by using the vfio-pci driver. There are several tutorials online that explain how to set up vfio-pci. It is important to note that some Nvidia GPUs expose additional PCI devices (such as an audio or usb controller) in the same IOMMU group as the GPU device itself. When binding the GPU device to the vfio-pci driver, ensure that any additional devices belonging to the GPU are bound as well.","title":"Cluster GPU Configuration"},{"location":"setup/gpu/#automatic-gpu-operator-configuration","text":"Opni can utilize GPU acceleration to enable log anomaly detection for workload and application logs. If you would like Opni to learn from your own workloads, follow the instructions below to configure your cluster. If GPU acceleration is not enabled, Opni can still analyze your Control Plane logs using our pretrained models. Opni bundles its own modified version of the Nvidia GPU Operator which is the recommended way to enable GPU acceleration. It may be possible to install the upstream GPU Operator via helm chart, but this is not recommended, as it will likely not work with Opni. Follow the instructions below to enable GPU acceleration using Opni's built-in GPU Operator.","title":"Automatic GPU Operator Configuration"},{"location":"setup/gpu/#prerequisites","text":"","title":"Prerequisites"},{"location":"setup/gpu/#gpu-node-hardware-requirements","text":"1x NVIDIA GPU If not using VGPU, any Quadro or Tesla GPU will work. If using VGPU, one of the following GPUs is required: Tesla M6, M10, M60 Tesla P4, P6, P40, P100 Tesla V100 Quadro RTX 6000/8000 Tesla T4 NVIDIA A10, A16, A30, A40, A100, RTX A5000/A6000 MIG is not supported when using automatic GPU Operator configuration at this time. Ubuntu 20.04. No other host OS is supported at this time. The GPU node should not have an Nvidia driver or container runtime installed. The GPU Operator handles installing all the necessary drivers automatically.","title":"GPU Node Hardware Requirements:"},{"location":"setup/gpu/#kubernetes-cluster-requirements","text":"Vanilla upstream Kubernetes, or one of the following distributions: RKE K3S >= v1.22.2+k3s1 RKE2 Amazon EKS Google GKE Azure AKS Opni Operator installed Cert-Manager installed NFD installed (see deploy/examples/nfd_aio.yaml in the Opni repo)","title":"Kubernetes Cluster Requirements:"},{"location":"setup/gpu/#install-custom-resources","text":"","title":"Install Custom Resources"},{"location":"setup/gpu/#gpupolicyadapter","text":"Creating a GPUPolicyAdapter resource will trigger the GPU Operator to start configuring the cluster. If not using VGPU, the GPUPolicyAdapter resource does not need any customization, and can be created with an empty spec, as follows: apiVersion : opni.io/v1beta1 kind : GpuPolicyAdapter metadata : name : gpu spec : {} (note that the empty brackets after spec: are important) If using VGPU, please follow the official documentation to build the VGPU driver image, and create a GPUPolicyAdapter resource with the following spec, filling in the driver and licenseConfigMap fields: apiVersion : opni.io/v1beta1 kind : GpuPolicyAdapter metadata : name : vgpu spec : images : driver : # Image name for the VGPU driver vgpu : licenseConfigMap : # ConfigMap containing gridd.conf and the client config token licenseServerKind : nls Once the GPUPolicyAdapter resource is created, the GPU Operator will begin configuring the cluster. This takes a few minutes, and the container engine will be restarted during this process. Once all the pods in the gpu-operator-resources namespace complete, GPUs will be available for use and the Opni GPU Controller pod should start.","title":"GPUPolicyAdapter"},{"location":"setup/gpu/#provider-specific-notes-and-troubleshooting","text":"","title":"Provider-specific Notes and Troubleshooting"},{"location":"setup/gpu/#k3s","text":"If you are using K3S, you must use v1.22.2+k3s1 or later. This version introduces automatic detection of Nvidia container runtimes on nodes, and a large number of bugfixes related to pod lifecycle were added in upstream Kubernetes for this release. If you are using an earlier version, you may experience issues where pods become stuck in the 'Terminating' state. If this happens, force-deleting the stuck pods should help resolve the issue.","title":"K3S"},{"location":"setup/gpu/#rke","text":"RKE uses Docker as its container engine. If you are using RKE, you should be aware of how the Nvidia runtime interacts with Docker. When the GPU operator is installed for the first time, it will create a RuntimeClass object which allows Kubernetes to use the nvidia runtime, which is configured in the docker engine itself. The RuntimeClass will look similar to this when using RKE: apiVersion : node.k8s.io/v1alpha1 kind : RuntimeClass metadata : name : nvidia handler : docker The RuntimeClass name is used in a pod spec to identify the runtime to use. It is specified using the runtimeClassName field: apiVersion : v1 kind : Pod spec : # ... runtimeClassName : nvidia # ... When using containerd for example, the handler field in the RuntimeClass is used to select a specific container runtime by name. However, dockershim does not support selecting custom container runtimes, meaning the nvidia container runtime must be set as the default. The GPU Operator will still utilize a RuntimeClass , but its handler must be set to docker to work. Otherwise, you may see an error like this: Failed to create pod sandbox: rpc error: code = Unknown desc = RuntimeHandler \"nvidia\" not supported If you encounter this error, check the handler field in the RuntimeClass and ensure it is set to docker .","title":"RKE"},{"location":"setup/gpu/#vgpu","text":"If you are using VGPUs, you should be aware of the following: Nvidia gridd does not run on the virtualization host. Instead, it runs in guest VMs - when using the GPU Operator, gridd runs inside the nvidia driver daemonset pod. The VGPU driver must match the driver on the host. When downloading the driver package from the Nvidia License Portal, you will be presented with two driver runfiles, for example: NVIDIA-Linux-x86_64-470.63-vgpu-kvm.run - Installed on the host NVIDIA-Linux-x86_64-470.63.01-grid.run - Installed on the guest (automatic with GPU operator) Ensure the correct drivers are used for the host and guest. CUDA drivers do not need to be installed on the virtualization host (and are not included by default with the VGPU driver) VGPU mediated device UUIDs are not persistent across reboots. This can cause libvirt to fail after a reboot if you are running a VM with VGPUs. To fix this: List instances on the host with virsh list --all (the instance will be shut off) Find the instance that has the VGPU. If you don't know which one it is, use the virsh edit command to view the XML configuration of each instance and search for mdev . Undefine the instance using virsh undefine and restart libvirtd. You might not be able to determine from the host whether a guest VGPU is licensed. To check license status, you should open a shell to a pod running with the nvidia container runtime and use nvidia-smi from there. On the virtualization host, you may notice that your physical GPU is bound to the nvidia driver, even though the nvidia_vgpu_vfio driver is available. The official docs are ambiguous on this, but your GPU should be bound to the nvidia driver. The guest VGPU will also be bound to the nvidia driver once the guest drivers are installed.","title":"VGPU"},{"location":"setup/gpu/#other-notes","text":"The nvidia.com/gpu resource request behaves counterintuitively. It serves as a node scheduling hint for pods, and to keep track of which/how many pods are using GPUs. nvidia.com/gpu a first-class node resource just like cpu and memory. If a pod makes a request for one, the node it is scheduled on must have that resource available. The existence of this resource request does not equate to a pod taking exclusive ownership of the device. In fact, a pod can use a GPU without requesting an nvidia.com/gpu resource at all, as long as it has the correct environment variables and runtimeClassName set. This is how the GPU Operator pods are configured - they do not request GPU resources by design, rather they are scheduled onto GPU nodes by other means and set up to use the nvidia container runtime. Regardless of nvidia.com/gpu resource requests or container runtime, GPUs will only be available if the NVIDIA_VISIBLE_DEVICES and NVIDIA_DRIVER_CAPABILITIES environment variables are set correctly. They should be set as follows: NVIDIA_VISIBLE_DEVICES=all NVIDIA_DRIVER_CAPABILITIES=compute,utility They can be set either in the pod spec, or in the docker image itself by using ENV in the Dockerfile. If you are not using the GPU operator, but you are using the nvidia device plugin daemonset by itself, be aware that you must patch the daemonset to include runtimeClassName: nvidia in its pod template. The GPU operator will do this automatically, but the daemonset does not have it set by default. The GPU Operator installs the drivers in such a way that they are not directly visible or usable by the host. As such, tools like nvidia-smi will not be available, so you will need to exec into a pod running with the nvidia container runtime to interact with a GPU. A common way to pass a GPU through to a virtual machine is by using the vfio-pci driver. There are several tutorials online that explain how to set up vfio-pci. It is important to note that some Nvidia GPUs expose additional PCI devices (such as an audio or usb controller) in the same IOMMU group as the GPU device itself. When binding the GPU device to the vfio-pci driver, ensure that any additional devices belonging to the GPU are bound as well.","title":"Other Notes"},{"location":"setup/log-shipping/","text":"The Opni payload receiver service provides a http endpoint for receiving JSON formatted logs from the FluentD http output. The recommended mechanism for configuring FluentD is to use the Banzaicloud logging operator Log Adapter \u00b6 The Opni operator understands the banzaicloud logging custom resources, but under the logging.opni.io API group instead of logging.banzaicloud.io . This allows the Opni operator to manage log shipping configuration, without conflicting with a separate Banzaicloud operator install. For convenience Opni has a LogAdapter custom resource for configuring log shipping on various Kubernetes distributions. This creates a Logging resource, and also configures a fluentbit DaemonSet to pick up Kubernetes system logs from either file based logging or journald. More details on the LogAdapter can be found on the LogAdapter configuration page . If using a LogAdapter the spec.deployLogCollector field can be set to true on the OpniCluster resource. This will create a ClusterOutput and ClusterFlow to ship the logs to the install Opni payload receiver service. Separate Logging Operator \u00b6 If using a Rancher distribution of Kubernetes you can use the Rancher Logging integration to ship logs. Once the Rancher Logging App is installed you will need to add a ClusterFlow and ClusterOutput to ship the logs to Opni: clusteroutput.yaml apiVersion : logging.banzaicloud.io/v1beta1 kind : ClusterOutput metadata : name : <clusteroutput name> spec : http : buffer : chunk_limit_size : 1mb flush_interval : 2s tags : '[]' timekey : \"\" content_type : application/json endpoint : <opni payload-receiver endpoint> json_array : true clusterflow.yaml apiVersion : logging.banzaicloud.io/v1beta1 kind : ClusterFlow metadata : name : <clusterflow name> spec : filters : - dedot : de_dot_nested : true de_dot_separator : '-' - grep : exclude : - key : log pattern : ^\\n$ - detectExceptions : languages : - java - python - go - ruby - js - csharp - php multiline_flush_interval : \"0.1\" globalOutputRefs : - <clusteroutput name> match : - exclude : namespaces : - <opni namespace> #We exclude the opni cluster namespace - select : {}","title":"Log Shipping"},{"location":"setup/log-shipping/#log-adapter","text":"The Opni operator understands the banzaicloud logging custom resources, but under the logging.opni.io API group instead of logging.banzaicloud.io . This allows the Opni operator to manage log shipping configuration, without conflicting with a separate Banzaicloud operator install. For convenience Opni has a LogAdapter custom resource for configuring log shipping on various Kubernetes distributions. This creates a Logging resource, and also configures a fluentbit DaemonSet to pick up Kubernetes system logs from either file based logging or journald. More details on the LogAdapter can be found on the LogAdapter configuration page . If using a LogAdapter the spec.deployLogCollector field can be set to true on the OpniCluster resource. This will create a ClusterOutput and ClusterFlow to ship the logs to the install Opni payload receiver service.","title":"Log Adapter"},{"location":"setup/log-shipping/#separate-logging-operator","text":"If using a Rancher distribution of Kubernetes you can use the Rancher Logging integration to ship logs. Once the Rancher Logging App is installed you will need to add a ClusterFlow and ClusterOutput to ship the logs to Opni: clusteroutput.yaml apiVersion : logging.banzaicloud.io/v1beta1 kind : ClusterOutput metadata : name : <clusteroutput name> spec : http : buffer : chunk_limit_size : 1mb flush_interval : 2s tags : '[]' timekey : \"\" content_type : application/json endpoint : <opni payload-receiver endpoint> json_array : true clusterflow.yaml apiVersion : logging.banzaicloud.io/v1beta1 kind : ClusterFlow metadata : name : <clusterflow name> spec : filters : - dedot : de_dot_nested : true de_dot_separator : '-' - grep : exclude : - key : log pattern : ^\\n$ - detectExceptions : languages : - java - python - go - ruby - js - csharp - php multiline_flush_interval : \"0.1\" globalOutputRefs : - <clusteroutput name> match : - exclude : namespaces : - <opni namespace> #We exclude the opni cluster namespace - select : {}","title":"Separate Logging Operator"},{"location":"setup/metrics/","text":"Metrics anomaly detection in Opni depends on having a Prometheus instance set up, and collecting metrics from the Kubernetes cluster. The Opni metrics service exposes the predictions via a Prometheus metrics enddpoint so this also needs to be regularly scraped. The simplest way to set up Prometheus is to use prometheus-operator . This can be installed with the community helm chart , or using the Rancher Monitoring application. Configuring Metrics. \u00b6 The Opni metrics service can be configured to work with prometheus-operator. To enable this provide a reference to the Prometheus resource you want to use. Opni will then use that to obtain the Prometheus URL, and create a ServiceMonitor and PrometheusRule (for alerts). If the Prometheus object doesn't include an externalURL field, or you are manually managing Prometheus, you will need to provide the Prometheus URL. If don't provide the Prometheus reference you will need to manually set up the Prometheus scrape config.","title":"Metrics Anomaly Detection (beta)"},{"location":"setup/metrics/#configuring-metrics","text":"The Opni metrics service can be configured to work with prometheus-operator. To enable this provide a reference to the Prometheus resource you want to use. Opni will then use that to obtain the Prometheus URL, and create a ServiceMonitor and PrometheusRule (for alerts). If the Prometheus object doesn't include an externalURL field, or you are manually managing Prometheus, you will need to provide the Prometheus URL. If don't provide the Prometheus reference you will need to manually set up the Prometheus scrape config.","title":"Configuring Metrics."},{"location":"setup/setup-grafana-dashboard/","text":"The Opni system supports an experimental version of metric anomaly detection service. To start get insights from it, its Grafana dashboard needs to be setup as follow: Navigate to Grafana and log in. For a Rancher Monitoring user, the default username/password is admin/prom-operator . Otherwise, it is likely to be admin/admin . Add Elasticsearch as a Data Source , and fill in these fields with following values: URL: https://opni-es-client.opni.svc:9200 Basic Auth: enable Skip TLS Verify: enable Basic Auth Details: User: admin Password: <opni-es-password> Index name: mymetrics Time field name: timestamp Version: 7.0+ Then click Save & Test . The expected response should be Index OK. Time field name OK. Import dashboard and upload the json file grafana-dashboard.json in this repo. A dashboard named MetricAnomaly should now be available.","title":"Setup Grafana Dashboard for Metric Service"}]}